# Configuration for training a Transformer model

tokenizer:
  vocab_path: "data/vocab.json"
  merges_path: "data/merges.json"
  special_tokens: ["<|endoftext|>"]

model:
  vocab_size: 1000
  context_length: 1024
  d_model: 512
  num_layers: 6
  num_heads: 8
  d_ff: 2048
  rope_theta: 10000

optimizer:
  lr_max: 1e-3
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.01

scheduler:
  T_w: 1000
  T_c: 10000
  lr_min: 1e-4

data:
  batch_size: 128
  device: "cuda"
  path: "data/TinyStoriesV2-GPT4-train.txt"
  tokenized_path: "data/TinyStoriesV2-GPT4-train-tokenized.pt"

training:
  max_iterations: 10000
  max_l2_norm: 1.0
  seed: 42
  log_interval: 100

wandb:
  project: "cs336-assignment1"
  name: "cs336-assignment1-run"

checkpoint:
  save_dir: "checkpoints"
  save_interval: 1000
  load_dir: "checkpoints"
  load_iteration: 0