# Configuration for training a Transformer model

tokenizer:
  vocab_path: "/root/cs336-assignment1/data/vocab.json"
  merges_path: "/root/cs336-assignment1/data/merges.json"
  special_tokens: ["<|endoftext|>"]

model:
  vocab_size: 10000
  batch_size: 16
  device: "cuda"
  context_length: 256
  d_model: 512
  num_layers: 6
  num_heads: 8
  d_ff: 2048
  rope_theta: 10000

optimizer:
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.01
scheduler:
  T_w: 10
  T_c: 900
  lr_max: 1e-3
  lr_min: 1e-4
data:
  path: "/root/cs336-assignment1/data/TinyStoriesV2-GPT4-train.txt"
  tokenized_path: "/root/cs336-assignment1/data/TinyStoriesV2-GPT4-train-tokenized.bin"
  num_processes: 10

training:
  max_iterations: 1000
  max_l2_norm: 1.0
  seed: 42
  log_interval: 100

wandb:
  project: "cs336-assignment1"
  name: "cs336-assignment1-run"

checkpoint:
  save_dir: "checkpoints"
  save_interval: 1000
  load_dir: "checkpoints"
  load_iteration: 0